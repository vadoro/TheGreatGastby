{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-1nzll33F2s",
    "outputId": "41a0d671-e6c1-41a5-a5b0-bb8cf92613d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwgJ9Miy0jww",
    "outputId": "45cd2120-76f3-4ea8-def5-7c980933d998"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "# 불용어 다운로드\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # 영어 불용어 집합\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KtxviNY3Z43"
   },
   "outputs": [],
   "source": [
    "data_dir = '/content/drive/MyDrive/논문/The Great Gatsby/Data_Gatsby/character_speech'\n",
    "output_dir = '/content/drive/MyDrive/논문/The Great Gatsby/Data_Gatsby/character_speech/matrix'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQc-MBS88Kjw"
   },
   "outputs": [],
   "source": [
    "def create_cooccurrence_matrix(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    content = data['content']\n",
    "\n",
    "    words_per_sentence = []\n",
    "    for c in content:\n",
    "      content_string = str(c)\n",
    "      sentences = nltk.sent_tokenize(content_string)\n",
    "      for sentence in sentences:\n",
    "        cleaned_sentence = re.sub(r'[^\\w\\s]',' ', sentence)   #알파벳(숫자 포함)과 공백 외의 모든 특수문자를 빈 칸으로 대체하고 제거\n",
    "        cleaned_sentence = re.sub(r'\\d+', '', cleaned_sentence)  # 숫자 제거\n",
    "        tokens = word_tokenize(cleaned_sentence.lower())  # 소문자로 변환하고 토큰화\n",
    "\n",
    "        new_tokens = []\n",
    "        for t in tokens:\n",
    "          lemmatized_token = lemmatizer.lemmatize(t)\n",
    "          if len(lemmatized_token) > 2 and lemmatized_token not in stop_words:\n",
    "            new_tokens.append(lemmatized_token)\n",
    "\n",
    "        words_per_sentence.append(new_tokens)\n",
    "\n",
    "    all_words = set() \n",
    "    for words in words_per_sentence:\n",
    "        all_words.update(words)\n",
    "\n",
    "    # 단어들을 알파벳 순으로 정렬\n",
    "    all_words = sorted(all_words)     \n",
    "\n",
    "    matrix = pd.DataFrame(0, index = all_words, columns = all_words)\n",
    "\n",
    "    for words in words_per_sentence:\n",
    "        for i, word1 in enumerate(words):        \n",
    "            for word2 in words[i+1:]:          \n",
    "                matrix.loc[word1, word2] += 1\n",
    "                matrix.loc[word2, word1] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def save_matrix_to_csv(matrix, output_file, suffix=\"_matrix\"):\n",
    "    # Create the output file name by appending the suffix and .csv extension\n",
    "    output_file = f\"{output_file}{suffix}.csv\"\n",
    "    matrix.to_csv(output_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jh095Hiu9ZFE",
    "outputId": "01b12fac-e91f-47de-a396-558fb1e4b761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Father_speech.csv\n",
      "Processing AuntsandUncles_speech.csv\n",
      "Processing man_speech.csv\n",
      "Processing Tom_speech.csv\n",
      "Processing Daisy_speech.csv\n",
      "Processing Nick_speech.csv\n",
      "Processing Jordan_speech.csv\n",
      "Processing MrWilson_speech.csv\n",
      "Processing MrsWilson_speech.csv\n",
      "Processing oldman_speech.csv\n",
      "Processing MrsMckee_speech.csv\n",
      "Processing MrMckee_speech.csv\n",
      "Processing Catherine_speech.csv\n",
      "Processing elevatorboy_speech.csv\n",
      "Processing twogirls_speech.csv\n",
      "Processing thegirl_speech.csv\n",
      "Processing Lucille_speech.csv\n",
      "Processing othergirl_speech.csv\n",
      "Processing MrMumble_speech.csv\n",
      "Processing owlman_speech.csv\n",
      "Processing Gatsby_speech.csv\n",
      "Processing orchestraleader_speech.csv\n",
      "Processing butler_speech.csv\n",
      "Processing agirl_speech.csv\n",
      "Processing wife1_speech.csv\n",
      "Processing wife2_speech.csv\n",
      "Processing wife3_speech.csv\n",
      "Processing husband2_speech.csv\n",
      "Processing husband3_speech.csv\n",
      "Processing theman_speech.csv\n",
      "Processing crowd_speech.csv\n",
      "Processing youngladies_speech.csv\n",
      "Processing policeman_speech.csv\n",
      "Processing Wolfshiem_speech.csv\n",
      "Processing waiter_speech.csv\n",
      "Processing officers_speech.csv\n",
      "Processing Ewing_speech.csv\n",
      "Processing reporter_speech.csv\n",
      "Processing awoman_speech.csv\n",
      "Processing Sloane_speech.csv\n",
      "Processing Baedeker_speech.csv\n",
      "Processing woman2_speech.csv\n",
      "Processing friend_speech.csv\n",
      "Processing Civet_speech.csv\n",
      "Processing WomanNexttoNick_speech.csv\n",
      "Processing conductor_speech.csv\n",
      "Processing Pammy_speech.csv\n",
      "Processing nurse_speech.csv\n",
      "Processing Michaelis_speech.csv\n",
      "Processing blackman_speech.csv\n",
      "Processing gardener_speech.csv\n",
      "Processing information_speech.csv\n",
      "Processing Slagle_speech.csv\n",
      "Processing Henry_speech.csv\n",
      "Processing Klipspringer_speech.csv\n",
      "Processing Stella_speech.csv\n",
      "Processing someone_speech.csv\n"
     ]
    }
   ],
   "source": [
    "# matrix.csv 파일 생성\n",
    "for file_name in [f for f in os.listdir(data_dir) if f.endswith('.csv')]:\n",
    "  print(f\"Processing {file_name}\")\n",
    "  matrix = create_cooccurrence_matrix(os.path.join(data_dir, file_name))\n",
    "  output_file = os.path.join(output_dir, file_name.split('.')[0])\n",
    "  save_matrix_to_csv(matrix, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krnYvITuXhTn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
